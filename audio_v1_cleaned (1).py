# -*- coding: utf-8 -*-
"""Audio_V1_Cleaned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rwz4pMKOLi4s7JELWuNVaLzlwYLq9Gar
"""

import numpy as np
import librosa
import os
import numpy as np
import scipy
from scipy.io import wavfile
import scipy.fftpack as fft
from scipy.signal import get_window
import IPython.display as ipd
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import librosa
import numpy as np
import librosa.display, os
import matplotlib.pyplot as plt
import librosa
import numpy as np
import random
from keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import classification_report
import librosa
from keras.preprocessing import image

if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print("GPU not found. Using CPU.")

def time_stretch(y,sr, speed_factor):
    y_stretched = librosa.effects.time_stretch(y, rate=speed_factor)
    return y_stretched, sr

def pitch_shift(y,sr, n_steps):
    y_shifted = librosa.effects.pitch_shift(y,sr=sr, n_steps= n_steps)
    return y_shifted, sr

def add_noise(audio, noise_factor):
    noise = np.random.randn(len(audio))
    y_noisy = audio + noise_factor * noise
    return y_noisy

def split_audio(audio, sr, duration=1):
    n_samples_per_segment = sr * duration
    n_segments = len(audio) // n_samples_per_segment
    segments = [audio[i*n_samples_per_segment:(i+1)*n_samples_per_segment] for i in range(n_segments)]
    return segments

def create_spectrogram(y,sr, image_file):
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ms = librosa.feature.melspectrogram(y=y, sr=sr)
    log_ms = librosa.power_to_db(ms, ref=np.max)
    librosa.display.specshow(log_ms, sr=sr)

    fig.savefig(image_file)
    plt.close(fig)

def create_pngs_from_wavs(input_path, output_path,emotion,index):
    if not os.path.exists(output_path):
        os.makedirs(output_path)

    dir = os.listdir(input_path)
    for i, file in enumerate(dir):
        input_file = os.path.join(input_path, file)
        output_file = os.path.join(output_path, emotion + str(index) +'.png')
        y, sr = librosa.load(audio_file)
        create_spectrogram(y,sr, output_file)

def create_dataset_Ravdess(initial_path,output_path):
  # initial_path = "/kaggle/input/ravdess-emotional-speech-audio"
  # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).
  Emotions = ["Neutral","Happy","Sad","Angry","Fearful"]
  emotions =  ["Neutral","Calm","Happy","Sad","Angry","Fearful","Disgust","Surprised"]
  speakers_2 = []
  for i in range(1,10):
      speakers_2.append("Actor_0" + str(i))
  for i in range(10,25):
      speakers_2.append("Actor_" + str(i))
  # /kaggle/input/ravdess-emotional-speech-audio/Actor_01
  for k in Emotions:
      index = 1
      # output_path = "/kaggle/working/" + k
      output_path = output_path + k
      speed_timee = [0.5,0.6,0.7,0.8,0.9,1.1,1.2,1.5,1.8,2.0]

      if not os.path.exists(output_path):
          os.makedirs(output_path)
      for i in speakers_2:
              directory_path = initial_path + "/" + i
              for audio_emo in os.listdir(directory_path):
      #             /kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav
                  if emotions[int(audio_emo[7])-1] == k :
                      audio_path = directory_path + "/" + audio_emo
                      y , i_sr = librosa.load(audio_path)
                      segments = split_audio(y,i_sr,1)
                      for segment in segments:
                          output_file = os.path.join(output_path, k + str(index) +'_original.png')
                          create_spectrogram(segment,i_sr, output_file)
                          index = index + 1

                          ## speed augmentation
                          speed = random.randint(0, 9)
                          segment_stretched, sr = time_stretch(segment, i_sr, speed_timee[speed])
                          output_file = os.path.join(output_path, k + str(index) + '_speed.png')
                          create_spectrogram(segment_stretched, sr, output_file)
                          index += 1

                          # Pitch augmentation
                          n_steps = random.randint(-3, 3)
                          segment_pitch_shifted, sr = pitch_shift(segment, i_sr, n_steps)
                          output_file = os.path.join(output_path, k + str(index) + '_pitch.png')
                          create_spectrogram(segment_pitch_shifted, sr, output_file)
                          index += 1

def create_dataset_IISC_ISER(initial_path,output_path):
  # initial_path = "/kaggle/input/indian-emotional-speech-corpora-iesc/Indian Emotional Speech Corpora (IESC)"
  speakers = ["Speaker-1","Speaker-2","Speaker-3","Speaker-4","Speaker-5","Speaker-6","Speaker-7","Speaker-8"]
  Emotions = ["Anger","Fear","Happy","Neutral","Sad"]
  for i in speakers:
      for j in Emotions:
          index = 1
          directory_path = initial_path + "/" + i +"/" + j
          if j == "Anger":
              j = "Angry"
          elif j == "Fear":
              j = "Fearful"
          # output_path = "/kaggle/working/" + j
          output_path = output_path + j
          for audio_emo in os.listdir(directory_path):
              audio_path = os.path.join(directory_path, audio_emo)
              y, sr = librosa.load(audio_path)

              # Split audio into 1-second segments
              segments = split_audio(y, sr,1)

              for segment in segments:
                  # Original spectrogram
                  output_file = os.path.join(output_path, f"{j }_{i}_SER{index}_original.png")
                  create_spectrogram(segment, sr, output_file)
                  index += 1

                  # Speed augmentation
                  speed = random.randint(0, 9)
                  segment_stretched, sr = time_stretch(segment, sr, speed_timee[speed])
                  output_file = os.path.join(output_path, f"{j }_{i}_SER{index}_speed.png")
                  create_spectrogram(segment_stretched, sr, output_file)
                  index += 1

                  # Pitch augmentation
                  n_steps = random.randint(-3, 3)
                  segment_pitch_shifted, sr = pitch_shift(segment, sr, n_steps)
                  output_file = os.path.join(output_path, f"{j }_{i}_SER{index}_pitch.png")
                  create_spectrogram(segment_pitch_shifted, sr, output_file)
                  index += 1

def load_images_from_path(path, label):
    images = []
    labels = []

    for file in os.listdir(path):
        images.append(image.img_to_array(image.load_img(os.path.join(path, file), target_size=(224, 224, 3))))
        labels.append((label))

    return images, labels

def show_images(images):
    fig, axes = plt.subplots(1, 8, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})

    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i] / 255)

def create_final_dataset():
  x = []
  y = []

  class_to_labels =["Fearful","Neutral","Happy","Sad","Angry"]

  images, labels = load_images_from_path('/kaggle/working/Fearful', 0)
  show_images(images)

  x += images
  y += labels

  images, labels = load_images_from_path('/kaggle/working/Neutral', 1)
  show_images(images)

  x += images
  y += labels

  images, labels = load_images_from_path('/kaggle/working/Happy', 2)
  show_images(images)

  x += images
  y += labels

  images, labels = load_images_from_path('/kaggle/working/Sad', 3)
  show_images(images)

  x += images
  y += labels

  images, labels = load_images_from_path('/kaggle/working/Angry', 4)
  show_images(images)

  x += images
  y += labels

  return x,y

x,y= create_final_dataset()
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=0)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, test_size=0.2, random_state=0)
x_train_norm = np.array(x_train) / 255
x_val_norm = np.array(x_val) / 255
x_test_norm = np.array(x_test) / 255
y_train_encoded = to_categorical(y_train)
y_val_encoded = to_categorical(y_val)
y_test_encoded = to_categorical(y_test)

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)

class CustomCNN:
    def __init__(self):
        self.model = self.build_model()

    def build_model(self):
        model = Sequential()
        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
        model.add(MaxPooling2D(2, 2))
        model.add(Conv2D(128, (3, 3), activation='relu'))
        model.add(MaxPooling2D(2, 2))
        model.add(Conv2D(128, (3, 3), activation='relu'))
        model.add(MaxPooling2D(2, 2))
        model.add(Conv2D(128, (3, 3), activation='relu'))
        model.add(MaxPooling2D(2, 2))
        model.add(Flatten())
        model.add(Dense(1024, activation='relu'))
        model.add(Dense(5, activation='softmax'))
        optimizer = Adam(learning_rate=0.001)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
        return model

    def train(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=32, filepath="model_checkpoint.keras"):
        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
        history = self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), callbacks=[checkpoint])
        return history

    def summary(self):
        self.model.summary()

custom_cnn = CustomCNN()
custom_cnn.summary()
history = custom_cnn.train(x_train_norm, y_train_encoded, x_val_norm, y_val_encoded, epochs=100, batch_size=32, filepath="model_checkpoint.keras")

def plot_training_history(history):
    # Plot training and validation accuracy
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    epochs = range(1, len(acc) + 1)

    plt.plot(epochs, acc, '-', label='Training Accuracy')
    plt.plot(epochs, val_acc, ':', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(loc='lower right')
    plt.show()

    # Plot training and validation loss
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    plt.plot(epochs, loss, '-', label='Training Loss')
    plt.plot(epochs, val_loss, ':', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='upper right')
    plt.show()

def evaluate_model(model, x_data, y_data):
    # Evaluate the model on test data
    loss, accuracy = model.evaluate(x_data, y_data)
    print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')

    # Predict on the data
    y_pred = model.predict(x_data)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # Calculate precision, recall, and F1-score
    target_names = ["Fearful", "Neutral", "Happy", "Sad", "Angry"]
    print(classification_report(np.argmax(y_data, axis=1), y_pred_classes, target_names=target_names))