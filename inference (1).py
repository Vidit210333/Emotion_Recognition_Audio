# -*- coding: utf-8 -*-
"""inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aMAJ8L1SOB63113B67V4I4zZjxX1mmlD
"""

import numpy as np
import librosa
import os
from keras.preprocessing import image
from keras.models import load_model
import matplotlib.pyplot as plt
import librosa.display

class EmotionRecognizer:
    def __init__(self, model_path):
        self.loaded_model = load_model(model_path)
        self.class_to_labels = ["Fearful", "Neutral", "Happy", "Sad", "Angry"]

    def create_spectrogram_audio_break(self, y, sr, image_file):
        fig = plt.figure()
        ax = fig.add_subplot(1, 1, 1)
        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
        ms = librosa.feature.melspectrogram(y=y, sr=sr)
        log_ms = librosa.power_to_db(ms, ref=np.max)
        librosa.display.specshow(log_ms, sr=sr)
        fig.savefig(image_file)
        plt.close(fig)

    def recognize_emotions(self, frames, sampling_rate,output_file):
        emotions = []
        for i, frame in enumerate(frames):
            self.create_spectrogram_audio_break(frame, sampling_rate, output_file+f"sampletest{i}.png")
            x = image.load_img(output_file+f"sampletest{i}.png", target_size=(224, 224))
            x = image.img_to_array(x)
            x = x.reshape((1,) + x.shape)  # Add batch dimension
            predictions = self.loaded_model.predict(x)
            for j, label in enumerate(self.class_to_labels):
                if predictions[0][j] == 1:
                    print("Frame", i, "Emotion:", label)
                    emotions.append(label)
        return emotions

    def break_audio_into_frames(self, audio_file, frame_duration=3):
        # Load the audio file
        y, sr = librosa.load(audio_file, sr=None)
        frame_length = sr * frame_duration
        frames = []
        for i in range(0, len(y), frame_length):
            frame = y[i:i+frame_length]
            frames.append(frame)

        return frames, sr

# Example usage
if __name__ == "__main__":
    model_path = "model_checkpoint.keras"
    recognizer = EmotionRecognizer(model_path)
    audio_file = '/kaggle/input/tim-sample/Timothe Chalamet - Miss Stevens award winning monologue.mp3'
    frames, sampling_rate = recognizer.break_audio_into_frames(audio_file)
    emotions = recognizer.recognize_emotions(frames, sampling_rate,output_file)